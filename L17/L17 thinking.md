## 1.假设输入是100*100的灰度图像，现在你使用卷积对图像进行特征提取，有50个滤波器，每个卷积核是5\*5大小，那么在隐藏层会有多少参数（需要考虑bias参数）
答：卷积层计算公式：

$$\begin{cases}
height_{out}= (height_{in}-height_{kernel}+2*padding)/stride + 1 \\
width_{out}= (width_{in}-width_{kernel}+2*padding)/stride + 1
\end{cases}$$

假设stride=1，padding=0，$height_{in}$和$width_{in}$都为100，$height_{kernal}=5$，由上面式子可以得到  96\*96\*50 尺寸的输出 ， 权重参数量为  5\*5\*50+50


## 2.局部不变性和参数共享指的是什么？
答：局部不变性是由滤波器模板决定的，如果滤波器模板为5*5，则认为在CNN前向传播过程中，保留 5\*5以内的局部空间联系，每个神经元只感受局部的图像区域，称为局部不变性；参数共享指的是隐层的参数个数和隐层的神经元个数无关，而是由滤波器的大小和滤波器种类有关系，即每一个局部区域共享的是同一套滤波器参数。


## 3.为什么会用到batch normalization ?
答：网络训练过程中参数不断改变导致每一层输入的分布也发生变化，而学习的过程又要使每一层适应输入的分布，因此训练的时候不得不降低学习率，小心初始化；而一般在训练网络会将输入均值，有些时候对输入做白化操作加速训练，所以这就提示我们可以在网络训练过程中也进行类似的操作，这就是batch normalization，通过规范化的手段，将每层神经网路的输入分布强行拉回到均值为0方差为1的标准正态分布，使得非线性变换函数的输入值落入到对输入敏感的区域，避免梯度消失问题，同时加快收敛。


## 4.使用dropout可以解决什么问题？
答：减少过拟合


## 5.ResNet中的Residual Block解决了什么问题？
答：残差学习 Residual Learning，解决CNN的效果退化问题，使得网络越深，准确率越高；Residual Block的设计，使让模型内部的结构具有恒等映射的能力，即使后面的网络没有学习到什么，也不会使网络表现更差。
