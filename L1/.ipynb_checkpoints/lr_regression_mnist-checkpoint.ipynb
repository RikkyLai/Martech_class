{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1144c6400>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1147e5be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 数据阅览\n",
    "# digits？\n",
    "digits= load_digits()\n",
    "data = digits.data\n",
    "print(data.shape)\n",
    "print(digits.images[0])\n",
    "print(digits.target[0])\n",
    "plt.gray()\n",
    "plt.title(\"Handwriting Digits\")\n",
    "plt.imshow(digits.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据归一化\n",
    "train_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)\n",
    "ss = preprocessing.StandardScaler()\n",
    "train_ss_x = ss.fit_transform(train_x)\n",
    "test_ss_x = ss.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 准确率：0.9600\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_ss_x, train_y)\n",
    "predict_y = lr.predict(test_ss_x)\n",
    "print(\"LR 准确率：%0.4lf\" % accuracy_score(predict_y, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_scaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Logistic Regression (aka logit, MaxEnt) classifier.\n",
       "\n",
       "In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
       "scheme if the 'multi_class' option is set to 'ovr', and uses the cross-\n",
       "entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
       "(Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
       "'sag' and 'newton-cg' solvers.)\n",
       "\n",
       "This class implements regularized logistic regression using the\n",
       "'liblinear' library, 'newton-cg', 'sag' and 'lbfgs' solvers. It can handle\n",
       "both dense and sparse input. Use C-ordered arrays or CSR matrices\n",
       "containing 64-bit floats for optimal performance; any other input format\n",
       "will be converted (and copied).\n",
       "\n",
       "The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
       "with primal formulation. The 'liblinear' solver supports both L1 and L2\n",
       "regularization, with a dual formulation only for the L2 penalty.\n",
       "\n",
       "Read more in the :ref:`User Guide <logistic_regression>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "penalty : str, 'l1' or 'l2', default: 'l2'\n",
       "    Used to specify the norm used in the penalization. The 'newton-cg',\n",
       "    'sag' and 'lbfgs' solvers support only l2 penalties.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
       "\n",
       "dual : bool, default: False\n",
       "    Dual or primal formulation. Dual formulation is only implemented for\n",
       "    l2 penalty with liblinear solver. Prefer dual=False when\n",
       "    n_samples > n_features.\n",
       "\n",
       "tol : float, default: 1e-4\n",
       "    Tolerance for stopping criteria.\n",
       "\n",
       "C : float, default: 1.0\n",
       "    Inverse of regularization strength; must be a positive float.\n",
       "    Like in support vector machines, smaller values specify stronger\n",
       "    regularization.\n",
       "\n",
       "fit_intercept : bool, default: True\n",
       "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
       "    added to the decision function.\n",
       "\n",
       "intercept_scaling : float, default 1.\n",
       "    Useful only when the solver 'liblinear' is used\n",
       "    and self.fit_intercept is set to True. In this case, x becomes\n",
       "    [x, self.intercept_scaling],\n",
       "    i.e. a \"synthetic\" feature with constant value equal to\n",
       "    intercept_scaling is appended to the instance vector.\n",
       "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
       "\n",
       "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
       "    as all other features.\n",
       "    To lessen the effect of regularization on synthetic feature weight\n",
       "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
       "\n",
       "class_weight : dict or 'balanced', default: None\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    If not given, all classes are supposed to have weight one.\n",
       "\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "\n",
       "    Note that these weights will be multiplied with sample_weight (passed\n",
       "    through the fit method) if sample_weight is specified.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *class_weight='balanced'*\n",
       "\n",
       "random_state : int, RandomState instance or None, optional, default: None\n",
       "    The seed of the pseudo random number generator to use when shuffling\n",
       "    the data.  If int, random_state is the seed used by the random number\n",
       "    generator; If RandomState instance, random_state is the random number\n",
       "    generator; If None, the random number generator is the RandomState\n",
       "    instance used by `np.random`. Used when ``solver`` == 'sag' or\n",
       "    'liblinear'.\n",
       "\n",
       "solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},\n",
       "    default: 'liblinear'\n",
       "    Algorithm to use in the optimization problem.\n",
       "\n",
       "    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
       "        'saga' are faster for large ones.\n",
       "    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
       "        handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
       "        schemes.\n",
       "    - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
       "        'liblinear' and 'saga' handle L1 penalty.\n",
       "\n",
       "    Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
       "    features with approximately the same scale. You can\n",
       "    preprocess the data with a scaler from sklearn.preprocessing.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       Stochastic Average Gradient descent solver.\n",
       "    .. versionadded:: 0.19\n",
       "       SAGA solver.\n",
       "\n",
       "max_iter : int, default: 100\n",
       "    Useful only for the newton-cg, sag and lbfgs solvers.\n",
       "    Maximum number of iterations taken for the solvers to converge.\n",
       "\n",
       "multi_class : str, {'ovr', 'multinomial'}, default: 'ovr'\n",
       "    Multiclass option can be either 'ovr' or 'multinomial'. If the option\n",
       "    chosen is 'ovr', then a binary problem is fit for each label. Else\n",
       "    the loss minimised is the multinomial loss fit across\n",
       "    the entire probability distribution. Does not work for liblinear\n",
       "    solver.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
       "\n",
       "verbose : int, default: 0\n",
       "    For the liblinear and lbfgs solvers set verbose to any positive\n",
       "    number for verbosity.\n",
       "\n",
       "warm_start : bool, default: False\n",
       "    When set to True, reuse the solution of the previous call to fit as\n",
       "    initialization, otherwise, just erase the previous solution.\n",
       "    Useless for liblinear solver.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
       "\n",
       "n_jobs : int, default: 1\n",
       "    Number of CPU cores used when parallelizing over classes if\n",
       "    multi_class='ovr'\". This parameter is ignored when the ``solver``is set\n",
       "    to 'liblinear' regardless of whether 'multi_class' is specified or\n",
       "    not. If given a value of -1, all cores are used.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "\n",
       "coef_ : array, shape (1, n_features) or (n_classes, n_features)\n",
       "    Coefficient of the features in the decision function.\n",
       "\n",
       "    `coef_` is of shape (1, n_features) when the given problem\n",
       "    is binary.\n",
       "\n",
       "intercept_ : array, shape (1,) or (n_classes,)\n",
       "    Intercept (a.k.a. bias) added to the decision function.\n",
       "\n",
       "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
       "    `intercept_` is of shape(1,) when the problem is binary.\n",
       "\n",
       "n_iter_ : array, shape (n_classes,) or (1, )\n",
       "    Actual number of iterations for all classes. If binary or multinomial,\n",
       "    it returns only 1 element. For liblinear solver, only the maximum\n",
       "    number of iteration across all classes is given.\n",
       "\n",
       "See also\n",
       "--------\n",
       "SGDClassifier : incrementally trained logistic regression (when given\n",
       "    the parameter ``loss=\"log\"``).\n",
       "sklearn.svm.LinearSVC : learns SVM models using the same algorithm.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The underlying C implementation uses a random number generator to\n",
       "select features when fitting the model. It is thus not uncommon,\n",
       "to have slightly different results for the same input data. If\n",
       "that happens, try with a smaller tol parameter.\n",
       "\n",
       "Predict output may not match that of standalone liblinear in certain\n",
       "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
       "in the narrative documentation.\n",
       "\n",
       "References\n",
       "----------\n",
       "\n",
       "LIBLINEAR -- A Library for Large Linear Classification\n",
       "    http://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
       "\n",
       "SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
       "    Minimizing Finite Sums with the Stochastic Average Gradient\n",
       "    https://hal.inria.fr/hal-00860051/document\n",
       "\n",
       "SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
       "    SAGA: A Fast Incremental Gradient Method With Support\n",
       "    for Non-Strongly Convex Composite Objectives\n",
       "    https://arxiv.org/abs/1407.0202\n",
       "\n",
       "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
       "    methods for logistic regression and maximum entropy models.\n",
       "    Machine Learning 85(1-2):41-75.\n",
       "    http://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
       "\u001b[0;31mFile:\u001b[0m           /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
