{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Thinking1：机器学习中的监督学习、非监督学习、强化学习有何区别\n",
    "答：监督学习是有training set 和label给予机器反馈，然后机器迭代更新自己的参数，使得结果和label越来越相似；而非监督学习是完全没有label，只有数据特征，根据特征的某种相似性，比如数据样本间的欧式距离、特征分布进行聚类学习；强化学习也没有label值，但它有奖励值，这个奖励值与label值不一样，是延后给出的，是在实践中学习，比如学习自行车和走路那样，只有实践后，看结果好不好，不好则给予负面奖励，好则给予正面奖励。\n",
    "\n",
    "\n",
    "\n",
    "## Thinking2：什么是策略网络，价值网络，有何区别\n",
    "答：策略网络是给定特定的输入，通过学习给出一个确定输出的网络；价值网络就是通过计算目前状态s的累积分数的期望，价值网络给游戏中的状态赋予一个数值/分数。 策略网络的输出是一个落子概率分布，价值网络掐、输出一个获胜的数值，用来调整网络的权重来逼近每一种棋局的真实输赢预测\n",
    "\n",
    "\n",
    "\n",
    "## Thinking3：请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion,Simluation，Backpropagation是如何操作的\n",
    "答：蒙特卡洛树的原理就是采样越多，越近似最优解。selection从根节点往下走，每次选“最有价值的子节点”，直到找到“存在未扩展的子节点”，即这个局面存在未走过的后续节点；expansion，给这个节点加上一个子节点；simulation，用快速走子的策略走到底，得到一个胜负结果；backpropagation，把模拟的结果驾到它的所有父节点上。\n",
    "\n",
    "\n",
    "\n",
    "## Thinking4：假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑\n",
    "答：推荐系统有以下的挑战：1.视频推荐的动态性，每天有很多新的视频上传或者用户经常改变偏好；2.大多推荐系统只是单纯把用户点击或未点击作为评级反馈；3.被推荐条目的多样性，使用强化学习使得系统能够通过自我学习更新网络的权重，从而达到更好的推荐效果。强化学习首先需要定义环境、智能体、状态、动作、奖赏，要考虑如何构建特征来定义动作和状态，用户特征、视频特征、时间特征等；定义奖赏，用户点击、用户活跃度、用户看视频的频率，策略网络得到候选视频概率等。\n",
    "\n",
    "\n",
    "\n",
    "## Thinking5：在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路\n",
    "答：agent是汽车，action就是汽车下一步前行的方向，state是当前的方向，reward是到达最后的地方是正反馈，如果找错方向或者撞到其它东西为负反馈，这一系列的状态动作作为时序输入到网络中训练，reward的值来更新调整网络d\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
