{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import scipy.io as scio\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = 'cars_annos.mat'\n",
    "# data = scio.loadmat(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'annotations', 'class_names'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(data['annotations'][0, 1][0][0]).convert('RGB')\n",
    "im.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[112, 7, 853, 717], [48, 24, 441, 202], [7, 4, 277, 180]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = []\n",
    "for i in range(3):\n",
    "    anno = []\n",
    "    for j in range(1, 5):\n",
    "        anno.append(int(data['annotations'][0, i][j]))\n",
    "    a.append(anno)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import os\n",
    "import time\n",
    "\n",
    "class CarDataset(Dataset):\n",
    "    def __init__(self, datafile, transform=None, train=False):\n",
    "        data = scio.loadmat(datafile)\n",
    "        self.transform = transform\n",
    "        self.annotations = []\n",
    "        self.classes = []\n",
    "        self.img_paths = []\n",
    "        for i in range(data['annotations'].shape[1]):\n",
    "            anno = []\n",
    "            if train and int(data['annotations'][0, i][6]) == 0:\n",
    "                for j in range(1, 5):\n",
    "                    anno.append(int(data['annotations'][0, i][j]))\n",
    "                self.annotations.append(anno)\n",
    "                self.classes.append(int(data['annotations'][0, i][5]))\n",
    "                self.img_paths.append(data['annotations'][0, i][0][0])\n",
    "            elif not train and int(data['annotations'][0, i][6]):\n",
    "                for j in range(1, 5):\n",
    "                    anno.append(int(data['annotations'][0, i][j]))\n",
    "                self.annotations.append(anno)\n",
    "                self.classes.append(int(data['annotations'][0, i][5]))\n",
    "                self.img_paths.append(data['annotations'][0, i][0][0])\n",
    "                 \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_paths[index]\n",
    "        label = self.classes[index]\n",
    "        annotation = self.annotations[index]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            label = int(label)\n",
    "        return img, label-1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "img_transform = transforms.Compose([\n",
    "            transforms.Resize((256,256),interpolation=3),\n",
    "            transforms.RandomCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "train_dataset = CarDataset(datafile, transform=img_transform, train=True)\n",
    "test_dataset = CarDataset(datafile, transform=img_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Epoch [0]  Loss: 0.6676 Acc: 0.0052 Time: 520.2124s\n",
      "train Epoch [1]  Loss: 0.6553 Acc: 0.0092 Time: 517.7003s\n",
      "train Epoch [2]  Loss: 0.6506 Acc: 0.0102 Time: 518.8676s\n",
      "train Epoch [3]  Loss: 0.6498 Acc: 0.0114 Time: 518.5370s\n",
      "train Epoch [4]  Loss: 0.6478 Acc: 0.0120 Time: 521.7381s\n",
      "train Epoch [5]  Loss: 0.6447 Acc: 0.0103 Time: 519.4752s\n",
      "train Epoch [6]  Loss: 0.6436 Acc: 0.0118 Time: 519.1273s\n",
      "train Epoch [7]  Loss: 0.6433 Acc: 0.0135 Time: 522.7507s\n",
      "train Epoch [8]  Loss: 0.6398 Acc: 0.0141 Time: 517.7616s\n",
      "train Epoch [9]  Loss: 0.6376 Acc: 0.0146 Time: 503.0382s\n",
      "train Epoch [10]  Loss: 0.6344 Acc: 0.0177 Time: 433.6416s\n",
      "train Epoch [11]  Loss: 0.6319 Acc: 0.0196 Time: 433.4290s\n",
      "train Epoch [12]  Loss: 0.6257 Acc: 0.0260 Time: 433.1971s\n",
      "train Epoch [13]  Loss: 0.6177 Acc: 0.0269 Time: 433.7028s\n",
      "train Epoch [14]  Loss: 0.6095 Acc: 0.0357 Time: 432.7685s\n",
      "train Epoch [15]  Loss: 0.5997 Acc: 0.0415 Time: 431.9605s\n",
      "train Epoch [16]  Loss: 0.5871 Acc: 0.0519 Time: 433.0289s\n",
      "train Epoch [17]  Loss: 0.5737 Acc: 0.0548 Time: 432.3760s\n",
      "train Epoch [18]  Loss: 0.5563 Acc: 0.0713 Time: 432.3633s\n",
      "train Epoch [19]  Loss: 0.5400 Acc: 0.0830 Time: 432.6270s\n",
      "train Epoch [20]  Loss: 0.5205 Acc: 0.1058 Time: 432.1854s\n",
      "train Epoch [21]  Loss: 0.4957 Acc: 0.1235 Time: 432.6107s\n",
      "train Epoch [22]  Loss: 0.4763 Acc: 0.1467 Time: 432.7310s\n",
      "train Epoch [23]  Loss: 0.4526 Acc: 0.1728 Time: 432.2933s\n",
      "train Epoch [24]  Loss: 0.4320 Acc: 0.2021 Time: 431.9224s\n",
      "train Epoch [25]  Loss: 0.4039 Acc: 0.2397 Time: 432.2005s\n",
      "train Epoch [26]  Loss: 0.3731 Acc: 0.2812 Time: 428.3550s\n",
      "train Epoch [27]  Loss: 0.3463 Acc: 0.3135 Time: 427.3216s\n",
      "train Epoch [28]  Loss: 0.3203 Acc: 0.3605 Time: 431.2417s\n",
      "train Epoch [29]  Loss: 0.2926 Acc: 0.4037 Time: 432.2139s\n",
      "train Epoch [30]  Loss: 0.2680 Acc: 0.4537 Time: 432.0590s\n",
      "train Epoch [31]  Loss: 0.2477 Acc: 0.4859 Time: 431.6809s\n",
      "train Epoch [32]  Loss: 0.2268 Acc: 0.5201 Time: 432.6275s\n",
      "train Epoch [33]  Loss: 0.2079 Acc: 0.5611 Time: 432.6689s\n",
      "train Epoch [34]  Loss: 0.1931 Acc: 0.5819 Time: 433.8495s\n",
      "train Epoch [35]  Loss: 0.1757 Acc: 0.6186 Time: 432.8701s\n",
      "train Epoch [36]  Loss: 0.1644 Acc: 0.6314 Time: 433.1246s\n",
      "train Epoch [37]  Loss: 0.1500 Acc: 0.6679 Time: 432.4615s\n",
      "train Epoch [38]  Loss: 0.1369 Acc: 0.6943 Time: 432.7278s\n",
      "train Epoch [39]  Loss: 0.1284 Acc: 0.7084 Time: 432.7546s\n",
      "train Epoch [40]  Loss: 0.1146 Acc: 0.7394 Time: 432.8401s\n",
      "train Epoch [41]  Loss: 0.1084 Acc: 0.7450 Time: 431.9600s\n",
      "train Epoch [42]  Loss: 0.0983 Acc: 0.7716 Time: 432.6283s\n",
      "train Epoch [43]  Loss: 0.0906 Acc: 0.7898 Time: 432.9169s\n",
      "train Epoch [44]  Loss: 0.0856 Acc: 0.7997 Time: 432.7442s\n",
      "train Epoch [45]  Loss: 0.0782 Acc: 0.8179 Time: 432.4829s\n",
      "train Epoch [46]  Loss: 0.0746 Acc: 0.8245 Time: 432.5986s\n",
      "train Epoch [47]  Loss: 0.0671 Acc: 0.8420 Time: 432.4588s\n",
      "train Epoch [48]  Loss: 0.0605 Acc: 0.8563 Time: 432.3963s\n",
      "train Epoch [49]  Loss: 0.0545 Acc: 0.8721 Time: 432.3817s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zero\\pycharmprojects\\test\\venv\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "c:\\users\\zero\\pycharmprojects\\test\\venv\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "c:\\users\\zero\\pycharmprojects\\test\\venv\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "c:\\users\\zero\\pycharmprojects\\test\\venv\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "c:\\users\\zero\\pycharmprojects\\test\\venv\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MaxPool2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "c:\\users\\zero\\pycharmprojects\\test\\venv\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "c:\\users\\zero\\pycharmprojects\\test\\venv\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Bottleneck. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "c:\\users\\zero\\pycharmprojects\\test\\venv\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type AdaptiveAvgPool2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "c:\\users\\zero\\pycharmprojects\\test\\venv\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car_resnet.pkl saved\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "EPOCH = 50\n",
    "batch_size = 8\n",
    "num_class = 196\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True )\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# 替换 fc 符合自己的分类\n",
    "model = models.resnet50(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_class) \n",
    "\n",
    "#损失函数:这里用交叉熵\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#优化器 这里用SGD\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "#device : GPU or CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 训练\n",
    "for epoch in range(EPOCH):\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        # 计算损失函数\n",
    "        loss = criterion(outputs, labels)\n",
    "        # 清空上一轮梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 参数更新\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data).to(torch.float32)\n",
    "    batch_loss = running_loss / ((i+1)*batch_size)\n",
    "    batch_acc = running_corrects / ((i+1)*batch_size)\n",
    "    print('{} Epoch [{}]  Loss: {:.4f} Acc: {:.4f} Time: {:.4f}s'. \\\n",
    "                          format('train', epoch, batch_loss, batch_acc, time.time()-start_time))\n",
    "\n",
    "\n",
    "#保存训练模型\n",
    "file_name = 'car_resnet.pkl'\n",
    "torch.save(model, file_name)\n",
    "print(file_name+' saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000测试图像 准确率:54.5206%\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "img_transform = transforms.Compose([\n",
    "            transforms.Resize((256,256),interpolation=3),\n",
    "            transforms.RandomCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "test_dataset = CarDataset(datafile, transform=img_transform)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "file_name = 'car_resnet.pkl'\n",
    "# 测试\n",
    "model = torch.load(file_name)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "correct, total = 0, 0\n",
    "\n",
    "for data in test_loader:\n",
    "    images, labels = data\n",
    "    images, labels = images.cuda(), labels.cuda()\n",
    "    # 前向传播\n",
    "    out = model(images)\n",
    "    _, predicted = torch.max(out.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "#输出识别准确率\n",
    "print('10000测试图像 准确率:{:.4f}%'.format(100.0 * correct / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([178], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([196], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-82.7535, -73.4430, -62.4700, -57.8358, -64.8521, -59.4653, -65.2462,\n",
       "         -56.6374, -61.5434, -58.9353, -57.5317, -65.7276, -67.1602, -68.9190,\n",
       "         -60.6845, -63.8614, -76.4340, -73.7415, -64.8479, -61.3080, -69.1059,\n",
       "         -66.1126, -65.4822, -65.4555, -63.8386, -64.8420, -66.2069, -60.6348,\n",
       "         -63.1836, -61.8758, -65.8972, -62.3835, -70.1772, -64.2736, -60.4333,\n",
       "         -58.7898, -60.3093, -66.7532, -63.4566, -61.0303, -66.6297, -62.4869,\n",
       "         -66.9926, -60.7368, -65.1228, -59.4861, -62.0199, -63.9951, -73.0693,\n",
       "         -62.7425, -71.1742, -59.2207, -62.3429, -74.8161, -74.3066, -65.6202,\n",
       "         -67.2411, -58.1937, -67.4876, -63.3684, -64.6068, -64.8132, -76.5681,\n",
       "         -57.9894, -68.7152, -75.5912, -69.9301, -65.9906, -66.0114, -80.0260,\n",
       "         -78.3138, -68.1342, -67.5819, -68.4198, -79.0545, -75.5072, -69.9324,\n",
       "         -69.9396, -74.8502, -69.7273, -69.2101, -66.4308, -68.9045, -70.0837,\n",
       "         -72.5530, -72.1565, -73.2755, -70.4809, -65.8382, -67.8540, -79.8214,\n",
       "         -83.6539, -70.6814, -70.8848, -72.4172, -70.6592, -68.9813, -65.3599,\n",
       "         -63.4021, -65.5948, -66.3048, -62.6202, -63.6165, -62.3169, -63.3856,\n",
       "         -57.3317, -71.5038, -65.9788, -75.6510, -72.6431, -62.7720, -70.8606,\n",
       "         -67.4483, -72.7450, -72.2966, -71.1513, -72.8101, -59.0261, -70.3319,\n",
       "         -64.4985, -72.0865, -73.1600, -73.0969, -68.3724, -74.4275, -75.1327,\n",
       "         -62.3976, -67.2776, -61.9531, -62.8493, -56.9600, -69.2313, -60.7093,\n",
       "         -63.5503, -63.5224, -68.0198, -59.2094, -61.5951, -60.1336, -66.4027,\n",
       "         -60.3543, -58.9838, -61.6988, -74.0548, -62.1190, -73.7949, -70.6114,\n",
       "         -68.9571, -69.7467, -70.8310, -60.0447, -60.4699, -59.6938, -57.7118,\n",
       "         -65.2419, -69.3209, -74.0220, -63.7717, -63.7954, -69.9356, -61.9204,\n",
       "         -70.6251, -67.4746, -64.8495, -64.1633, -65.2878, -67.8283, -61.6646,\n",
       "         -58.9724, -73.5138, -61.5050, -67.0550, -69.9193, -58.2651, -70.2829,\n",
       "         -69.8483, -69.0381, -65.7554, -56.6320, -62.9537, -59.2444, -60.9289,\n",
       "         -65.1111, -62.7498, -63.3789, -58.2779, -66.2355, -65.7237, -66.2227,\n",
       "         -69.6944, -59.0557, -66.5352, -61.1552, -63.3900, -66.4572, -70.1121,\n",
       "         -57.4716]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步提高精度的方向：\n",
    "\n",
    "+ 增加epoch\n",
    "+ 根据提供的anno 进一步分割出 car的位置\n",
    "+ 类别是196，labels应该整体减1\n",
    "+ 图像增强\n",
    "\n",
    "需要确定是过拟合还是欠拟合，需要划分traindataset和valdataset 看loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
